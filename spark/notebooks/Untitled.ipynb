{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a68f3f-1388-4861-956f-079f6baa15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create session with spark with master spark and minio configuration\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret\", \"minioadmin12\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123456\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a465217-b949-44c0-a775-349ab89988b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc57b64-eaa9-42e2-897f-f1690ce483b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o57.conf.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:93)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:76)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:116)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sharedState$1(SparkSession.scala:175)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.classic.SparkSession.sharedState$lzycompute(SparkSession.scala:175)\n\tat org.apache.spark.sql.classic.SparkSession.sharedState(SparkSession.scala:174)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sessionState$2(SparkSession.scala:186)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.classic.SparkSession.sessionState$lzycompute(SparkSession.scala:184)\n\tat org.apache.spark.sql.classic.SparkSession.sessionState(SparkSession.scala:181)\n\tat org.apache.spark.sql.classic.SparkSession.conf$lzycompute(SparkSession.scala:197)\n\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:197)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/lib/python3.13/site-packages/IPython/core/formatters.py:406\u001B[39m, in \u001B[36mBaseFormatter.__call__\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    404\u001B[39m     method = get_real_method(obj, \u001B[38;5;28mself\u001B[39m.print_method)\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m406\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    407\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/pyspark/sql/session.py:681\u001B[39m, in \u001B[36mSparkSession._repr_html_\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    674\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_repr_html_\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    675\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m    676\u001B[39m \u001B[33m        <div>\u001B[39m\n\u001B[32m    677\u001B[39m \u001B[33m            <p><b>SparkSession - \u001B[39m\u001B[38;5;132;01m{catalogImplementation}\u001B[39;00m\u001B[33m</b></p>\u001B[39m\n\u001B[32m    678\u001B[39m \u001B[33m            \u001B[39m\u001B[38;5;132;01m{sc_HTML}\u001B[39;00m\n\u001B[32m    679\u001B[39m \u001B[33m        </div>\u001B[39m\n\u001B[32m    680\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\"\"\"\u001B[39m.format(\n\u001B[32m--> \u001B[39m\u001B[32m681\u001B[39m         catalogImplementation=\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconf\u001B[49m.get(\u001B[33m\"\u001B[39m\u001B[33mspark.sql.catalogImplementation\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    682\u001B[39m         sc_HTML=\u001B[38;5;28mself\u001B[39m.sparkContext._repr_html_(),\n\u001B[32m    683\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/conda/lib/python3.13/functools.py:1025\u001B[39m, in \u001B[36mcached_property.__get__\u001B[39m\u001B[34m(self, instance, owner)\u001B[39m\n\u001B[32m   1023\u001B[39m val = cache.get(\u001B[38;5;28mself\u001B[39m.attrname, _NOT_FOUND)\n\u001B[32m   1024\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n\u001B[32m-> \u001B[39m\u001B[32m1025\u001B[39m     val = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1026\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1027\u001B[39m         cache[\u001B[38;5;28mself\u001B[39m.attrname] = val\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/pyspark/sql/session.py:853\u001B[39m, in \u001B[36mSparkSession.conf\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    825\u001B[39m \u001B[38;5;129m@cached_property\u001B[39m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconf\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> RuntimeConfig:\n\u001B[32m    827\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Runtime configuration interface for Spark.\u001B[39;00m\n\u001B[32m    828\u001B[39m \n\u001B[32m    829\u001B[39m \u001B[33;03m    This is the interface through which the user can get and set all Spark and Hadoop\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    851\u001B[39m \u001B[33;03m    'value'\u001B[39;00m\n\u001B[32m    852\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m853\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m RuntimeConfig(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jsparkSession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:282\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpy4j\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprotocol\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m282\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    283\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    284\u001B[39m     converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    325\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    328\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    329\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    332\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    333\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling o57.conf.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:93)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:76)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:116)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sharedState$1(SparkSession.scala:175)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.classic.SparkSession.sharedState$lzycompute(SparkSession.scala:175)\n\tat org.apache.spark.sql.classic.SparkSession.sharedState(SparkSession.scala:174)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sessionState$2(SparkSession.scala:186)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.classic.SparkSession.sessionState$lzycompute(SparkSession.scala:184)\n\tat org.apache.spark.sql.classic.SparkSession.sessionState(SparkSession.scala:181)\n\tat org.apache.spark.sql.classic.SparkSession.conf$lzycompute(SparkSession.scala:197)\n\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:197)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f40acdd2990>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f89fa7-a0a7-43db-981b-f1798bc59dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
