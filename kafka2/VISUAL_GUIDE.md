# Visual Setup & Troubleshooting Guide

Complete visual diagrams and quick troubleshooting flowcharts.

---

## Architecture Diagram

### Complete Data Flow

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          CDC PIPELINE OVERVIEW                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YOUR WEB APPLICATION (nb-habit-helper)                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Create/Update/Delete Tasks                                        â”‚  â”‚
â”‚ â”‚ - User clicks "Add Task"                                          â”‚  â”‚
â”‚ â”‚ - Submits form to backend                                         â”‚  â”‚
â”‚ â”‚ - Backend saves to PostgreSQL                                     â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ PostgreSQL INSERT/UPDATE/DELETE
                                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SUPABASE POSTGRESQL DATABASE                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ public.tasks table                                                â”‚  â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚  â”‚
â”‚ â”‚ â”‚ id       â”‚ title    â”‚ completedâ”‚ due_date â”‚ ...                â”‚  â”‚
â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”‚  â”‚
â”‚ â”‚ â”‚ uuid1    â”‚ Task 1   â”‚ false    â”‚ 2026-01-15                    â”‚  â”‚
â”‚ â”‚ â”‚ uuid2    â”‚ Task 2   â”‚ true     â”‚ 2026-01-20                    â”‚  â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚ â”‚                                                                    â”‚  â”‚
â”‚ â”‚ âš™ï¸ Logical Replication ENABLED (wal_level=logical)               â”‚  â”‚
â”‚ â”‚ ğŸ“¢ Publication: debezium_publication FOR TABLE public.tasks      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ WAL (Write-Ahead Log)
                                       â”‚ Replication Slot
                                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEBEZIUM POSTGRESQL CDC CONNECTOR                          (Running on  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Docker) â”‚
â”‚ â”‚ Source: PostgreSQL                                                   â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚ â”‚ â”‚ â–¶ Connects to: db.xxxxx.supabase.co:5432                   â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ â–¶ Reads: publication debezium_publication                  â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ â–¶ Captures: INSERT, UPDATE, DELETE                         â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ â–¶ Snapshot: Initial full table scan                        â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ â–¶ After: Incremental changes (CDC)                         â”‚  â”‚  â”‚
â”‚ â”‚ â”‚                                                              â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ Process:                                                     â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ 1. Read replication slot (logical decoding)                â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ 2. Deserialize WAL messages                                â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ 3. Create Avro records                                     â”‚  â”‚  â”‚
â”‚ â”‚ â”‚ 4. Send to Kafka                                           â”‚  â”‚  â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ Avro + Schema Registry
                                       â”‚ (Serialized messages)
                                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APACHE KAFKA (Distributed Message Broker)                 (Running on   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Docker)     â”‚
â”‚ â”‚ Topic: supabase-habit.public.tasks                                   â”‚
â”‚ â”‚ Partitions: 1                                                        â”‚
â”‚ â”‚ Replication: 1                                                       â”‚
â”‚ â”‚                                                                      â”‚
â”‚ â”‚ Message Format:                                                      â”‚
â”‚ â”‚ {                                                                    â”‚
â”‚ â”‚   "before": null | {...old values...},                             â”‚
â”‚ â”‚   "after": {...new values...},                                     â”‚
â”‚ â”‚   "op": "c" | "u" | "d" | "r",  (create/update/delete/read)       â”‚
â”‚ â”‚   "ts_ms": 1705339482000,                                          â”‚
â”‚ â”‚   "source": {...CDC metadata...}                                   â”‚
â”‚ â”‚ }                                                                    â”‚
â”‚ â”‚                                                                      â”‚
â”‚ â”‚ Consumers:                                                           â”‚
â”‚ â”‚ â”œâ”€ connect-minio-s3-sink (S3 Sink Connector) â†’ LAG: 0              â”‚
â”‚ â”‚ â””â”€ Other applications (if connected)                               â”‚
â”‚ â”‚                                                                      â”‚
â”‚ â”‚ Schema Registry: http://schema-registry:8087                       â”‚
â”‚ â”‚ â”œâ”€ Subject: supabase-habit.public.tasks-value                      â”‚
â”‚ â”‚ â”œâ”€ Version: 1+                                                     â”‚
â”‚ â”‚ â””â”€ Type: AVRO                                                      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ Poll & Batch
                                       â”‚ (Avro messages)
                                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KAFKA S3 SINK CONNECTOR (Batch â†’ Parquet)            (Running on Docker) â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Source Topic: supabase-habit.public.tasks                         â”‚  â”‚
â”‚ â”‚ Destination: MinIO (S3-compatible)                                â”‚  â”‚
â”‚ â”‚                                                                    â”‚  â”‚
â”‚ â”‚ Process:                                                           â”‚  â”‚
â”‚ â”‚ 1. Poll messages from Kafka (batch size: 1000)                   â”‚  â”‚
â”‚ â”‚ 2. Buffer in memory                                              â”‚  â”‚
â”‚ â”‚ 3. When batch full OR timeout (1 hour) â†’ flush                  â”‚  â”‚
â”‚ â”‚ 4. Convert Avro â†’ Parquet columnar format                       â”‚  â”‚
â”‚ â”‚ 5. Compress with Snappy                                         â”‚  â”‚
â”‚ â”‚ 6. Write to MinIO                                               â”‚  â”‚
â”‚ â”‚                                                                    â”‚  â”‚
â”‚ â”‚ Output path: s3://datalake/bronze/cdc/tasks/                    â”‚  â”‚
â”‚ â”‚             topics/supabase-habit.public.tasks/                 â”‚  â”‚
â”‚ â”‚             partition=0/000000000000000000_0.parquet            â”‚  â”‚
â”‚ â”‚                                                                    â”‚  â”‚
â”‚ â”‚ Consumer Group: connect-minio-s3-sink                           â”‚  â”‚
â”‚ â”‚ Status: RUNNING                                                  â”‚  â”‚
â”‚ â”‚ LAG: 0 (when caught up)                                         â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ Parquet files
                                       â”‚ (Columnar, compressed)
                                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MINIO (S3-COMPATIBLE DATA LAKE)                    (Running on Docker)   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Bucket: datalake                                                  â”‚  â”‚
â”‚ â”‚ â”œâ”€ bronze/                        (Bronze = Raw/Hot zone)         â”‚  â”‚
â”‚ â”‚ â”‚  â””â”€ cdc/                       (CDC data)                       â”‚  â”‚
â”‚ â”‚ â”‚     â””â”€ tasks/                  (Task events)                    â”‚  â”‚
â”‚ â”‚ â”‚        â””â”€ topics/                                              â”‚  â”‚
â”‚ â”‚ â”‚           â””â”€ supabase-habit.public.tasks/                      â”‚  â”‚
â”‚ â”‚ â”‚              â””â”€ partition=0/                                   â”‚  â”‚
â”‚ â”‚ â”‚                 â”œâ”€ 000000000000000000_0.parquet (12 KB)         â”‚  â”‚
â”‚ â”‚ â”‚                 â”œâ”€ 000000000000000001_0.parquet (14 KB)         â”‚  â”‚
â”‚ â”‚ â”‚                 â””â”€ 000000000000000002_0.parquet (11 KB)         â”‚  â”‚
â”‚ â”‚ â”‚                                                                 â”‚  â”‚
â”‚ â”‚ â”‚ Access Methods:                                                â”‚  â”‚
â”‚ â”‚ â”‚ 1. Web Console: http://localhost:9001 (minioadmin/minioadmin) â”‚  â”‚
â”‚ â”‚ â”‚ 2. AWS CLI: aws s3 ls s3://datalake/ --endpoint-url ...       â”‚  â”‚
â”‚ â”‚ â”‚ 3. Python: boto3 or pandas                                     â”‚  â”‚
â”‚ â”‚ â”‚ 4. SQL: Query via Presto/Trino                                â”‚  â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Parquet File Schema:                                              â”‚  â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚ â”‚ â”‚ Column       â”‚ Type        â”‚ Example                         â”‚ â”‚  â”‚
â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚
â”‚ â”‚ â”‚ __op         â”‚ string      â”‚ "c" (create)                    â”‚ â”‚  â”‚
â”‚ â”‚ â”‚ __ts_ms      â”‚ bigint      â”‚ 1705339482000                   â”‚ â”‚  â”‚
â”‚ â”‚ â”‚ __deleted    â”‚ boolean     â”‚ false                           â”‚ â”‚  â”‚
â”‚ â”‚ â”‚ before       â”‚ string      â”‚ null (JSON)                     â”‚ â”‚  â”‚
â”‚ â”‚ â”‚ after        â”‚ string      â”‚ {...task data...} (JSON)        â”‚ â”‚  â”‚
â”‚ â”‚ â”‚ source       â”‚ string      â”‚ {...CDC metadata...} (JSON)     â”‚ â”‚  â”‚
â”‚ â”‚ â”‚ ts_ms        â”‚ bigint      â”‚ 1705339482000                   â”‚ â”‚  â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Timeline Diagram

### What Happens When You Create a Task

```
YOUR APPLICATION
    â”‚
    â”‚ 1. User clicks "Add Task"
    â”œâ”€ Title: "My Task"
    â”œâ”€ Date: "2026-01-15"
    â””â”€ Clicks "Save" â†’ HTTP POST to backend

SUPABASE PostgreSQL
    â”‚
    â”‚ T+0.0s: INSERT INTO public.tasks (id, title, ...) VALUES (...)
    â”‚ T+0.0s: Write-Ahead Log (WAL) records change
    â”‚ T+0.0s: Replication slot receives WAL entry
    â”‚
    â””â”€â”€â†’ Debezium detects change

DEBEZIUM CDC CONNECTOR
    â”‚
    â”‚ T+0.5s: Read from replication slot (logical decoding)
    â”‚ T+0.7s: Deserialize WAL record to logical values
    â”‚ T+0.8s: Create Avro message:
    â”‚         {
    â”‚           "op": "c",  (CREATE)
    â”‚           "before": null,
    â”‚           "after": { id: "...", title: "My Task", ... },
    â”‚           "ts_ms": 1705339482000
    â”‚         }
    â”‚ T+0.9s: Register schema in Schema Registry (if new)
    â”‚ T+1.0s: Publish to Kafka
    â”‚
    â””â”€â”€â†’ Message in Kafka Topic

KAFKA (Topic: supabase-habit.public.tasks)
    â”‚
    â”‚ T+1.0s: Message offset #42 available
    â”‚ T+1.0s: S3 Sink connector polls Kafka
    â”‚ T+1.1s: Message deserialized from Avro
    â”‚ T+1.1s: Buffered in memory (batch: 1/1000)
    â”‚ T+1.1s: S3 Sink starts listening for more messages
    â”‚
    â””â”€â”€â†’ Waiting for more messages OR timeout

KAFKA S3 SINK CONNECTOR
    â”‚
    â”‚ Scenario A: Batch not full (< 1000 messages)
    â”‚ â””â”€ T+60min: Timeout! Flush buffered messages
    â”‚
    â”‚ Scenario B: Batch full (1000 messages)
    â”‚ â””â”€ T+1.5s-T+5s: Flush immediately
    â”‚
    â”œâ”€ T+3.0s (example): Flush triggered
    â”‚ T+3.1s: Convert buffered Avro â†’ Parquet columnar
    â”‚ T+3.2s: Compress with Snappy codec
    â”‚ T+3.3s: Write to MinIO
    â”‚ T+3.4s: Upload complete
    â”‚
    â””â”€â”€â†’ Parquet file in data lake

MINIO DATA LAKE
    â”‚
    â”‚ T+3.5s: File appears in bucket:
    â”‚         s3://datalake/bronze/cdc/tasks/
    â”‚         topics/supabase-habit.public.tasks/
    â”‚         partition=0/000000000000000000_0.parquet
    â”‚
    â”‚ File details:
    â”‚ - Size: ~50-100 KB (Snappy compressed)
    â”‚ - Format: Apache Parquet (columnar)
    â”‚ - Readable by: Python (pandas), SQL (Presto), etc.
    â”‚
    â””â”€â”€â†’ Available for analytics

OBSERVABLE IN:
    â”œâ”€ Web: http://localhost:9001
    â”œâ”€ CLI: aws s3 ls s3://datalake/... --endpoint-url http://localhost:9000
    â””â”€ Code: python -c "import pandas; pd.read_parquet(...)"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL TIME: 1-10 seconds (from task creation to data lake)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

## Troubleshooting Flowchart

### Quick Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CDC Pipeline Not Working?          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Docker       â”‚  docker compose ps
        â”‚ services OK? â”‚  (all "Up"?)
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
          YES â”‚    â”‚ NO
             â”‚    â””â”€â”€â†’ Run: docker compose up -d
             â”‚         Wait 30 seconds
             â”‚         Try again
             â”‚
             â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Debezium     â”‚  curl.exe http://localhost:8083/connectors/
      â”‚ connector    â”‚  supabase-postgres-cdc/status
      â”‚ RUNNING?     â”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
        YES â”‚    â”‚ NO (LOADING/FAILED)
           â”‚    â”‚
           â”‚    â””â”€â”€â†’ Issue: Check logs
           â”‚        docker compose logs kafka-connect
           â”‚
           â”‚        â”œâ”€â†’ "Connection refused"
           â”‚        â”‚   Fix: Verify Supabase hostname/password
           â”‚        â”‚        Edit: debezium-postgres-cdc.json
           â”‚        â”‚        Recreate connector
           â”‚        â”‚
           â”‚        â”œâ”€â†’ "wal_level is 'replica'"
           â”‚        â”‚   Fix: Need Supabase paid tier
           â”‚        â”‚        Contact Supabase support
           â”‚        â”‚
           â”‚        â””â”€â†’ "Publication not found"
           â”‚            Fix: Create in Supabase:
           â”‚                 CREATE PUBLICATION debezium_publication
           â”‚                 FOR TABLE public.tasks;
           â”‚
           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Kafka topic  â”‚  docker exec kafka-broker kafka-topics
      â”‚ created?     â”‚  --bootstrap-server kafka-broker:29092
      â”‚              â”‚  --list | grep supabase
      â”‚ (should see: â”‚
      â”‚ supabase-    â”‚
      â”‚ habit.public â”‚
      â”‚ .tasks)      â”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
        YES â”‚    â”‚ NO
           â”‚    â”‚
           â”‚    â””â”€â”€â†’ Debezium hasn't started yet
           â”‚        OR failed to initialize
           â”‚        â†’ Go back to "Debezium connector RUNNING?"
           â”‚
           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Messages in  â”‚  docker exec kafka-broker \
      â”‚ Kafka topic? â”‚  kafka-avro-console-consumer \
      â”‚              â”‚  --topic supabase-habit.public.tasks \
      â”‚ (should see  â”‚  --from-beginning --max-messages 5 \
      â”‚ some output) â”‚  --property schema.registry.url=...
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
        YES â”‚    â”‚ NO
           â”‚    â”‚
           â”‚    â””â”€â”€â†’ Debezium not capturing changes
           â”‚        
           â”‚        Possible causes:
           â”‚        â”œâ”€â†’ No changes made to PostgreSQL
           â”‚        â”‚   Fix: Create/update a task in your app
           â”‚        â”‚
           â”‚        â”œâ”€â†’ Replication slot not working
           â”‚        â”‚   Fix: Check in PostgreSQL:
           â”‚        â”‚        SELECT * FROM pg_replication_slots;
           â”‚        â”‚
           â”‚        â””â”€â†’ Debezium restarted
           â”‚            Fix: Check logs for errors
           â”‚                 docker compose logs kafka-connect
           â”‚
           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ S3 Sink      â”‚  curl.exe http://localhost:8083/connectors/
      â”‚ connector    â”‚  minio-s3-sink/status
      â”‚ RUNNING?     â”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
        YES â”‚    â”‚ NO
           â”‚    â”‚
           â”‚    â””â”€â”€â†’ Issue: Check logs
           â”‚        docker compose logs kafka-connect | grep minio
           â”‚
           â”‚        Possible causes:
           â”‚        â”œâ”€â†’ MinIO not accessible
           â”‚        â”‚   Fix: curl.exe http://localhost:9000/
           â”‚        â”‚        minio/health/live
           â”‚        â”‚
           â”‚        â””â”€â†’ Credentials wrong
           â”‚            Fix: Check minio-s3-sink.json
           â”‚                 Verify minioadmin/minioadmin
           â”‚
           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ MinIO bucket â”‚  aws s3 ls s3://datalake/ \
      â”‚ exists?      â”‚  --endpoint-url http://localhost:9000
      â”‚              â”‚
      â”‚ (should see: â”‚  OR: http://localhost:9001
      â”‚ datalake)    â”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
        YES â”‚    â”‚ NO
           â”‚    â”‚
           â”‚    â””â”€â”€â†’ Fix: Create bucket
           â”‚        aws s3 mb s3://datalake \
           â”‚        --endpoint-url http://localhost:9000
           â”‚
           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Parquet      â”‚  aws s3 ls s3://datalake/bronze/cdc/tasks/ \
      â”‚ files in     â”‚  --recursive --endpoint-url http://localhost:9000
      â”‚ MinIO?       â”‚
      â”‚              â”‚
      â”‚ (should see: â”‚  OR: http://localhost:9001 â†’ datalake
      â”‚ *.parquet)   â”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
        YES â”‚    â”‚ NO
           â”‚    â”‚
       âœ… SUCCESS! â””â”€â”€â†’ Issue: S3 Sink not writing files
           â”‚           
           â”‚           Possible causes:
           â”‚           â”œâ”€â†’ Consumer lag high
           â”‚           â”‚   Fix: Check:
           â”‚           â”‚        docker exec kafka-broker \
           â”‚           â”‚        kafka-consumer-groups \
           â”‚           â”‚        --group connect-minio-s3-sink \
           â”‚           â”‚        --describe
           â”‚           â”‚
           â”‚           â”œâ”€â†’ Flush size not reached
           â”‚           â”‚   Fix: Create more tasks OR adjust
           â”‚           â”‚        "flush.size": "100" in
           â”‚           â”‚        minio-s3-sink.json
           â”‚           â”‚
           â”‚           â””â”€â†’ Connector erroring silently
           â”‚               Fix: Check connector task state
           â”‚                    curl.exe http://localhost:8083/
           â”‚                    connectors/minio-s3-sink/status
           â”‚                    â†’ Look for "FAILED" in tasks
           â”‚
           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ âœ… WORKING!  â”‚
      â”‚ Create/      â”‚
      â”‚ Update/      â”‚
      â”‚ Delete tasks â”‚
      â”‚ and monitor  â”‚
      â”‚ MinIO files  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Docker Service States

### Healthy State

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service        â”‚ Expected Status                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ kafka-broker   â”‚ Up (healthy)                           â”‚
â”‚                â”‚ Port 29092 listening                   â”‚
â”‚                â”‚ KRaft mode active                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ cassandra      â”‚ Up (healthy)                           â”‚
â”‚                â”‚ Port 9042 listening                    â”‚
â”‚                â”‚ From Phase 1                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ schema-registryâ”‚ Up (healthy)                           â”‚
â”‚                â”‚ Port 8087 responding                   â”‚
â”‚                â”‚ Connected to Kafka                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ kafka-connect  â”‚ Up (healthy)                           â”‚
â”‚                â”‚ Port 8083 responding                   â”‚
â”‚                â”‚ Plugins installed:                     â”‚
â”‚                â”‚ - Debezium PostgreSQL                  â”‚
â”‚                â”‚ - Confluent S3 Sink                    â”‚
â”‚                â”‚ - DataStax Cassandra (from Phase 1)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ minio          â”‚ Up (healthy)                           â”‚
â”‚                â”‚ Port 9000 API responding              â”‚
â”‚                â”‚ Port 9001 web console accessible     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Connector State Machine

### Debezium CDC Connector

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ CREATED  â”‚  Created but not yet started
                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  LOADING     â”‚  Initializing, connecting to PostgreSQL
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                   â”‚     â”‚
            WAIT   â”‚     â”‚ ERROR
                   â”‚     â””â”€â”€â†’ âŒ FAILED â†’ Check logs
                   â”‚
                   â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  RUNNING     â”‚  âœ… Capturing changes
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚          â†‘
              â”‚ ERROR    â”‚ RESTART
              â†“          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  FAILED      â”‚  âŒ Crashed or error occurred
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### S3 Sink Connector

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ CREATED  â”‚  Created but not yet started
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  LOADING     â”‚  Initializing, connecting to Kafka
          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
               â”‚     â”‚
        WAIT   â”‚     â”‚ ERROR
               â”‚     â””â”€â”€â†’ âŒ FAILED
               â”‚
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  RUNNING     â”‚  âœ… Polling Kafka, buffering, writing to MinIO
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚          â†‘
          â”‚ ERROR    â”‚ RESTART
          â†“          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  FAILED      â”‚  âŒ Connection lost, permissions, etc.
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow Checkpoints

### Testing from Top to Bottom

```
Checkpoint 1: PostgreSQL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SELECT COUNT(*) FROM public.tasks;
Status: âœ… If records exist, PostgreSQL is working
Action: Create/update a task, verify row count increases


Checkpoint 2: Kafka Topic Exists
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
docker exec kafka-broker kafka-topics --bootstrap-server kafka-broker:29092 --list
Status: âœ… If "supabase-habit.public.tasks" appears
Action: If not, Debezium connector failed - check logs


Checkpoint 3: Kafka Has Messages
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
docker exec kafka-broker kafka-console-consumer \
  --topic supabase-habit.public.tasks \
  --from-beginning \
  --max-messages 1
Status: âœ… If you see binary output (Avro)
Action: If not, Debezium hasn't captured changes


Checkpoint 4: Kafka Has Readable Messages
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
docker exec kafka-broker kafka-avro-console-consumer \
  --topic supabase-habit.public.tasks \
  --from-beginning \
  --max-messages 1 \
  --property schema.registry.url=http://schema-registry:8087
Status: âœ… If you see JSON with "op", "before", "after"
Action: If not, Schema Registry issue


Checkpoint 5: Consumer Group LAG
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
docker exec kafka-broker kafka-consumer-groups \
  --group connect-minio-s3-sink \
  --describe
Status: âœ… If LAG is 0 or low (<100)
Action: If high, S3 Sink can't keep up


Checkpoint 6: MinIO Has Files
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
aws s3 ls s3://datalake/bronze/cdc/tasks/ --recursive \
  --endpoint-url http://localhost:9000 --no-sign-request
Status: âœ… If you see *.parquet files with timestamps
Action: If not, S3 Sink not writing


Checkpoint 7: Parquet Files Are Valid
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python -c "import pandas; print(pandas.read_parquet('file.parquet').shape)"
Status: âœ… If you see (rows, columns)
Action: If error, file might be corrupted
```

---

## Performance Monitoring

### Key Metrics to Track

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ METRIC                  â”‚ GOOD          â”‚ BAD               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Debezium Status         â”‚ RUNNING       â”‚ LOADING/FAILED    â•‘
â•‘ S3 Sink Status          â”‚ RUNNING       â”‚ LOADING/FAILED    â•‘
â•‘ Consumer LAG            â”‚ 0-10 msgs     â”‚ >1000 msgs        â•‘
â•‘ Kafka Messages/sec      â”‚ 1-10          â”‚ 0 (no changes)    â•‘
â•‘ Parquet File Size       â”‚ 1-100 KB      â”‚ >100 KB (uncompressed?) â•‘
â•‘ Time to MinIO (latency) â”‚ <10 seconds   â”‚ >60 seconds       â•‘
â•‘ Kafka Broker CPU        â”‚ <30%          â”‚ >80%              â•‘
â•‘ Kafka Broker Memory     â”‚ <50%          â”‚ >80%              â•‘
â•‘ MinIO Disk Used         â”‚ Growing       â”‚ Static (not writing) â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Check These Commands Regularly

```powershell
# 1. Overall health (every minute)
foreach ($i in 1..10) {
    $deb = curl.exe -s http://localhost:8083/connectors/supabase-postgres-cdc/status | ConvertFrom-Json
    $sink = curl.exe -s http://localhost:8083/connectors/minio-s3-sink/status | ConvertFrom-Json
    Write-Host "$(Get-Date) | Debezium: $($deb.connector.state) | S3 Sink: $($sink.connector.state)"
    Start-Sleep -Seconds 60
}

# 2. Consumer lag (watch growth)
docker exec kafka-broker kafka-consumer-groups \
  --bootstrap-server kafka-broker:29092 \
  --group connect-minio-s3-sink \
  --describe

# 3. MinIO file count (should grow)
aws s3 ls s3://datalake/bronze/cdc/tasks/ --recursive --endpoint-url http://localhost:9000 | wc -l

# 4. Docker stats (resource usage)
docker stats
```

---

## Emergency Restart Procedures

### If Something Goes Wrong

```
SCENARIO: Debezium connector crashed
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Check logs:
   docker compose logs kafka-connect --tail=50

2. Delete connector:
   curl.exe -X DELETE http://localhost:8083/connectors/supabase-postgres-cdc

3. Verify deleted:
   curl.exe http://localhost:8083/connectors | ConvertFrom-Json

4. Fix config (if needed):
   notepad debezium-postgres-cdc.json

5. Recreate:
   curl.exe -X POST -H "Content-Type: application/json" \
     --data "@debezium-postgres-cdc.json" \
     http://localhost:8083/connectors

6. Monitor:
   curl.exe http://localhost:8083/connectors/supabase-postgres-cdc/status


SCENARIO: S3 Sink lagging behind
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Check LAG:
   docker exec kafka-broker kafka-consumer-groups \
     --group connect-minio-s3-sink --describe

2. Increase flush speed (for low latency):
   Edit minio-s3-sink.json:
   "flush.size": "100"
   "rotate.interval.ms": "10000"

3. Recreate connector:
   curl.exe -X DELETE http://localhost:8083/connectors/minio-s3-sink
   curl.exe -X POST -H "Content-Type: application/json" \
     --data "@minio-s3-sink.json" \
     http://localhost:8083/connectors


SCENARIO: MinIO full or corrupted
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Check disk space:
   docker exec minio df -h

2. List files:
   aws s3 ls s3://datalake/ --recursive --endpoint-url http://localhost:9000

3. Delete old files:
   aws s3 rm s3://datalake/bronze/cdc/tasks/OLD_PATH --recursive \
     --endpoint-url http://localhost:9000

4. Restart MinIO:
   docker compose restart minio


SCENARIO: All services hung
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Full restart:
   docker compose down
   docker compose up -d
   Start-Sleep -Seconds 60

2. Verify:
   docker compose ps

3. Re-create connectors:
   ./start-cdc.ps1 (recommended)
   OR manually recreate both connectors
```

---

## Summary

This visual guide covers:

âœ… Complete architecture and data flow
âœ… Timeline of what happens when you create a task
âœ… Decision tree for troubleshooting
âœ… Docker service states
âœ… Connector state machines
âœ… Testing checkpoints
âœ… Performance metrics
âœ… Emergency restart procedures

Use the decision tree first when things don't work!

